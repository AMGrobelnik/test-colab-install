{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vld24p0hkai",
   "metadata": {},
   "source": [
    "# Pairwise PID Synergy Matrices on Benchmark Datasets\n",
    "\n",
    "This notebook computes **Partial Information Decomposition (PID)** synergy matrices on benchmark classification datasets. It decomposes the mutual information between feature pairs and a target variable into four atoms: **synergy**, **redundancy**, **unique information from feature 0**, and **unique information from feature 1**.\n",
    "\n",
    "**Key analyses:**\n",
    "1. **XOR Validation** — verifies PID computation on the canonical XOR distribution (expected synergy ≈ 1.0)\n",
    "2. **Pairwise Synergy Matrix** — computes PID for all feature pairs using BROJA or MMI estimators\n",
    "3. **Synergy vs MI Comparison** — measures Jaccard overlap between top synergy pairs and top MI features\n",
    "4. **Stability Analysis** — assesses reproducibility via subsampled Spearman correlations\n",
    "5. **Synergy Graph** — constructs a graph with edges between high-synergy feature pairs\n",
    "\n",
    "**Method:** PID_BROJA for small datasets (≤100 pairs), PID_MMI for larger ones, with Co-Information baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aioknwk41g7",
   "metadata": {},
   "outputs": [],
   "source": "import subprocess, sys\ndef _pip(*a): subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', *a])\n\n# Packages not pre-installed on Colab\n_pip('boltons')\n_pip('debtcollector')\n_pip('lattices')\n_pip('PLTable')\n_pip('--no-deps', 'dit==1.5')\n\n# Core packages (pre-installed on Colab, only needed locally)\nif 'google.colab' not in sys.modules:\n    _pip('pandas==2.2.2', 'scikit-learn==1.6.1', 'matplotlib==3.10.0', 'networkx==3.6.1', 'scipy==1.16.3')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jriit9xg7w",
   "metadata": {},
   "outputs": [],
   "source": "# NumPy 2.0 compat shim (dit uses removed np.alltrue/np.sometrue)\nimport numpy as np\nif not hasattr(np, \"alltrue\"): np.alltrue = np.all\nif not hasattr(np, \"sometrue\"): np.sometrue = np.any\n\nimport json\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom itertools import combinations\nfrom typing import Any\n\nimport dit\nfrom dit.pid import PID_BROJA, PID_MMI\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.feature_selection import mutual_info_classif\nfrom scipy.stats import spearmanr\nimport networkx as nx"
  },
  {
   "cell_type": "markdown",
   "id": "ivvii8a8t1",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load the mini demo dataset (iris, 90 balanced samples across 3 classes) from GitHub, with local fallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7i2ogh3y8f9",
   "metadata": {},
   "outputs": [],
   "source": "GITHUB_DATA_URL = \"https://raw.githubusercontent.com/AMGrobelnik/test-colab-install/master/pid_mini_demo_data.json\"\nimport json, os\n\ndef load_data():\n    try:\n        import urllib.request\n        with urllib.request.urlopen(GITHUB_DATA_URL) as response:\n            return json.loads(response.read().decode())\n    except Exception: pass\n    if os.path.exists(\"mini_demo_data.json\"):\n        with open(\"mini_demo_data.json\") as f: return json.load(f)\n    raise FileNotFoundError(\"Could not load mini_demo_data.json\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20yxnxq0kbw",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()\n",
    "print(f\"Loaded {len(data['datasets'])} dataset(s)\")\n",
    "for ds in data['datasets']:\n",
    "    print(f\"  {ds['dataset']}: {len(ds['examples'])} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o18kl9wiwv",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "All tunable parameters. Start with minimum values for quick testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ij3p7kgc57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Tunable parameters ──────────────────────────────────────────────\n",
    "N_BINS = 5                    # Discretization bins\n",
    "BROJA_PAIR_LIMIT = 100        # Max pairs for BROJA (else fallback to MMI)\n",
    "STABILITY_SUBSAMPLES = 5      # Number of subsamples for stability analysis\n",
    "STABILITY_FRACTION = 0.8      # Subsample fraction\n",
    "SYNERGY_THRESHOLD_PCTL = 75   # Percentile for synergy graph threshold\n",
    "MAX_FEATURES_LARGE = 30       # Subsample features for datasets with >30 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dip6s4gkih8",
   "metadata": {},
   "source": [
    "## Dataset Parsing\n",
    "\n",
    "Parse the loaded JSON into numpy arrays (X features, y labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o83ypwhmhsf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_datasets(raw_data: dict) -> dict[str, dict[str, Any]]:\n",
    "    \"\"\"Parse JSON datasets into numpy arrays {name: {X, y, feature_names, ...}}.\"\"\"\n",
    "    datasets = {}\n",
    "    for ds_entry in raw_data[\"datasets\"]:\n",
    "        ds_name = ds_entry[\"dataset\"]\n",
    "        examples = ds_entry[\"examples\"]\n",
    "        if len(examples) == 0:\n",
    "            continue\n",
    "\n",
    "        first_input = json.loads(examples[0][\"input\"])\n",
    "        feature_names = list(first_input.keys())\n",
    "        n_features = len(feature_names)\n",
    "\n",
    "        X_rows, y_vals = [], []\n",
    "        for ex in examples:\n",
    "            inp = json.loads(ex[\"input\"])\n",
    "            X_rows.append([float(inp[fn]) for fn in feature_names])\n",
    "            y_vals.append(str(ex[\"output\"]))\n",
    "\n",
    "        X = np.array(X_rows, dtype=np.float64)\n",
    "        unique_classes = sorted(set(y_vals))\n",
    "        class_map = {c: i for i, c in enumerate(unique_classes)}\n",
    "        y = np.array([class_map[v] for v in y_vals], dtype=np.int32)\n",
    "\n",
    "        n_classes = int(examples[0].get(\"metadata_n_classes\", len(unique_classes)))\n",
    "        datasets[ds_name] = {\n",
    "            \"X\": X, \"y\": y, \"feature_names\": feature_names,\n",
    "            \"n_classes\": n_classes, \"n_samples\": X.shape[0],\n",
    "            \"n_features\": n_features, \"class_labels\": unique_classes,\n",
    "        }\n",
    "        n_pairs = n_features * (n_features - 1) // 2\n",
    "        print(f\"  {ds_name:30s} | {X.shape[0]:5d} samples | \"\n",
    "              f\"{n_features:3d} features | {n_classes} classes | {n_pairs} pairs\")\n",
    "    return datasets\n",
    "\n",
    "datasets = parse_datasets(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ttq0h8lketl",
   "metadata": {},
   "source": [
    "## XOR Validation\n",
    "\n",
    "Validate the PID computation on the canonical XOR distribution. XOR has **pure synergy** (both inputs needed to predict output), so synergy should be ≈ 1.0 and redundancy ≈ 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ncot6dol8x",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor_validation() -> dict:\n",
    "    \"\"\"Validate PID computation on XOR distribution.\"\"\"\n",
    "    # XOR: Y = X1 XOR X2\n",
    "    xor_dist = dit.Distribution(\n",
    "        [\"000\", \"001\", \"010\", \"011\", \"100\", \"101\", \"110\", \"111\"],\n",
    "        [1 / 4, 0, 0, 1 / 4, 0, 1 / 4, 1 / 4, 0],\n",
    "    )\n",
    "    r_broja = PID_BROJA(xor_dist)\n",
    "    broja_syn = r_broja.get_pi(((0, 1),))\n",
    "    broja_red = r_broja.get_pi(((0,), (1,)))\n",
    "    broja_u0 = r_broja.get_pi(((0,),))\n",
    "    broja_u1 = r_broja.get_pi(((1,),))\n",
    "\n",
    "    r_mmi = PID_MMI(xor_dist)\n",
    "    mmi_syn = r_mmi.get_pi(((0, 1),))\n",
    "    mmi_red = r_mmi.get_pi(((0,), (1,)))\n",
    "\n",
    "    # AND: Y = X1 AND X2 (redundancy test)\n",
    "    and_dist = dit.Distribution(\n",
    "        [\"000\", \"010\", \"100\", \"111\"],\n",
    "        [1 / 4, 1 / 4, 1 / 4, 1 / 4],\n",
    "    )\n",
    "    r_and = PID_BROJA(and_dist)\n",
    "    and_syn = r_and.get_pi(((0, 1),))\n",
    "    and_red = r_and.get_pi(((0,), (1,)))\n",
    "\n",
    "    # Conservation check\n",
    "    total_pi = broja_syn + broja_red + broja_u0 + broja_u1\n",
    "\n",
    "    results = {\n",
    "        \"xor_broja_synergy\": round(float(broja_syn), 6),\n",
    "        \"xor_broja_redundancy\": round(float(broja_red), 6),\n",
    "        \"xor_broja_unique_0\": round(float(broja_u0), 6),\n",
    "        \"xor_broja_unique_1\": round(float(broja_u1), 6),\n",
    "        \"xor_mmi_synergy\": round(float(mmi_syn), 6),\n",
    "        \"xor_mmi_redundancy\": round(float(mmi_red), 6),\n",
    "        \"xor_conservation_total\": round(float(total_pi), 6),\n",
    "        \"and_broja_synergy\": round(float(and_syn), 6),\n",
    "        \"and_broja_redundancy\": round(float(and_red), 6),\n",
    "        \"xor_synergy_pass\": bool(abs(broja_syn - 1.0) < 0.01),\n",
    "        \"xor_redundancy_pass\": bool(abs(broja_red) < 0.01),\n",
    "        \"conservation_pass\": bool(abs(total_pi - 1.0) < 0.05),\n",
    "    }\n",
    "\n",
    "    print(f\"XOR synergy (BROJA):  {broja_syn:.4f} (expect ~1.0) {'PASS' if results['xor_synergy_pass'] else 'FAIL'}\")\n",
    "    print(f\"XOR redundancy:       {broja_red:.4f} (expect ~0.0) {'PASS' if results['xor_redundancy_pass'] else 'FAIL'}\")\n",
    "    print(f\"Conservation:         {total_pi:.4f} (expect ~1.0) {'PASS' if results['conservation_pass'] else 'FAIL'}\")\n",
    "    print(f\"AND synergy:          {and_syn:.4f}, AND redundancy: {and_red:.4f}\")\n",
    "    return results\n",
    "\n",
    "xor_results = xor_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vfdwhxu7vh",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Core computation functions: discretization, trivariate distribution building, PID synergy, and Co-Information baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7wgyr2hamst",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(X: np.ndarray, n_bins: int = N_BINS) -> np.ndarray:\n",
    "    \"\"\"Quantile-based discretization of continuous features.\"\"\"\n",
    "    X_disc = np.zeros_like(X, dtype=np.int32)\n",
    "    for col in range(X.shape[1]):\n",
    "        col_data = X[:, col]\n",
    "        n_unique = len(np.unique(col_data))\n",
    "        if n_unique <= 1:\n",
    "            X_disc[:, col] = 0\n",
    "        else:\n",
    "            actual_bins = min(n_bins, n_unique)\n",
    "            disc = KBinsDiscretizer(\n",
    "                n_bins=actual_bins, encode=\"ordinal\",\n",
    "                strategy=\"quantile\", subsample=None,\n",
    "            )\n",
    "            X_disc[:, col] = disc.fit_transform(col_data.reshape(-1, 1)).ravel().astype(np.int32)\n",
    "    return X_disc\n",
    "\n",
    "\n",
    "def build_trivariate_dist(xi: np.ndarray, xj: np.ndarray, y: np.ndarray):\n",
    "    \"\"\"Build a dit.Distribution from three discrete arrays.\"\"\"\n",
    "    counts = Counter(zip(xi.tolist(), xj.tolist(), y.tolist()))\n",
    "    total = sum(counts.values())\n",
    "    max_val = max(max(xi), max(xj), max(y))\n",
    "    if max_val >= 10:\n",
    "        outcomes = [(a, b, c) for (a, b, c) in counts.keys()]\n",
    "        probs = [count / total for count in counts.values()]\n",
    "        return dit.Distribution(outcomes, probs)\n",
    "    else:\n",
    "        outcomes = [f\"{a}{b}{c}\" for (a, b, c) in counts.keys()]\n",
    "        probs = [count / total for count in counts.values()]\n",
    "        return dit.Distribution(outcomes, probs)\n",
    "\n",
    "\n",
    "def compute_full_pid(xi: np.ndarray, xj: np.ndarray, y: np.ndarray, use_broja: bool = True) -> dict:\n",
    "    \"\"\"Compute full PID decomposition and return all atoms.\"\"\"\n",
    "    PID_Cls = PID_BROJA if use_broja else PID_MMI\n",
    "    d = build_trivariate_dist(xi, xj, y)\n",
    "    result = PID_Cls(d)\n",
    "    return {\n",
    "        \"synergy\": float(result.get_pi(((0, 1),))),\n",
    "        \"unique_0\": float(result.get_pi(((0,),))),\n",
    "        \"unique_1\": float(result.get_pi(((1,),))),\n",
    "        \"redundancy\": float(result.get_pi(((0,), (1,)))),\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_co_information(xi: np.ndarray, xj: np.ndarray, y: np.ndarray) -> float:\n",
    "    \"\"\"Compute co-information (interaction information) as synergy proxy.\n",
    "    Negative CoI indicates synergy. We return -CoI so positive = synergy.\"\"\"\n",
    "    mi_i = mutual_info_classif(xi.reshape(-1, 1), y, discrete_features=True, random_state=42)[0]\n",
    "    mi_j = mutual_info_classif(xj.reshape(-1, 1), y, discrete_features=True, random_state=42)[0]\n",
    "    X_pair = np.column_stack([xi, xj])\n",
    "    mi_pair = mutual_info_classif(X_pair, y, discrete_features=True, random_state=42)[0]\n",
    "    return -(mi_i + mi_j - mi_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fvilqj6t0bg",
   "metadata": {},
   "source": [
    "## Compute Pairwise Synergy Matrix\n",
    "\n",
    "For each feature pair, compute the full PID decomposition (synergy, redundancy, unique info) and the Co-Information baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "id69po55jek",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_synergy_matrix(\n",
    "    X_disc: np.ndarray, y: np.ndarray, feature_names: list[str],\n",
    "    dataset_name: str, feature_indices: list[int] | None = None,\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Compute pairwise PID synergy matrix for a dataset.\"\"\"\n",
    "    if feature_indices is None:\n",
    "        feature_indices = list(range(X_disc.shape[1]))\n",
    "\n",
    "    n_feat = len(feature_indices)\n",
    "    n_pairs = n_feat * (n_feat - 1) // 2\n",
    "    synergy_matrix = np.zeros((n_feat, n_feat), dtype=np.float64)\n",
    "    coi_matrix = np.zeros((n_feat, n_feat), dtype=np.float64)\n",
    "    pid_details = {}\n",
    "\n",
    "    use_broja = n_pairs <= BROJA_PAIR_LIMIT\n",
    "    pid_method = \"BROJA\" if use_broja else \"MMI\"\n",
    "    pairs = list(combinations(range(n_feat), 2))\n",
    "\n",
    "    print(f\"  {dataset_name}: {n_pairs} pairs, method={pid_method}\")\n",
    "\n",
    "    t_start = time.time()\n",
    "    completed, errors = 0, 0\n",
    "\n",
    "    for li, lj in pairs:\n",
    "        gi, gj = feature_indices[li], feature_indices[lj]\n",
    "        xi, xj = X_disc[:, gi], X_disc[:, gj]\n",
    "\n",
    "        try:\n",
    "            pid = compute_full_pid(xi, xj, y, use_broja=use_broja)\n",
    "            synergy_matrix[li, lj] = pid[\"synergy\"]\n",
    "            synergy_matrix[lj, li] = pid[\"synergy\"]\n",
    "            pair_key = f\"{feature_names[gi]}__x__{feature_names[gj]}\"\n",
    "            pid_details[pair_key] = pid\n",
    "            completed += 1\n",
    "        except Exception:\n",
    "            errors += 1\n",
    "\n",
    "        try:\n",
    "            coi_val = compute_co_information(xi, xj, y)\n",
    "            coi_matrix[li, lj] = coi_val\n",
    "            coi_matrix[lj, li] = coi_val\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    total_time = time.time() - t_start\n",
    "    print(f\"  {dataset_name}: {completed}/{n_pairs} pairs in {total_time:.1f}s ({errors} errors)\")\n",
    "\n",
    "    return {\n",
    "        \"synergy_matrix\": synergy_matrix.tolist(),\n",
    "        \"coi_matrix\": coi_matrix.tolist(),\n",
    "        \"pid_method\": pid_method,\n",
    "        \"n_pairs\": n_pairs, \"completed_pairs\": completed, \"errors\": errors,\n",
    "        \"total_time_s\": round(total_time, 2),\n",
    "        \"pid_details\": pid_details,\n",
    "        \"feature_indices_used\": feature_indices,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lzd8b0rl6ws",
   "metadata": {},
   "source": [
    "## Synergy vs MI Comparison\n",
    "\n",
    "Compare top synergy pairs against pairs formed from top MI features. Low Jaccard overlap confirms synergy captures different information than MI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l041apbgijm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_synergy_vs_mi(\n",
    "    synergy_matrix: np.ndarray, mi_values: np.ndarray,\n",
    "    feature_names: list[str], feature_indices: list[int], top_k: int = 10,\n",
    ") -> dict:\n",
    "    \"\"\"Compare top synergy pairs vs pairs formed from top MI features.\"\"\"\n",
    "    n_feat = len(feature_indices)\n",
    "    pairs = list(combinations(range(n_feat), 2))\n",
    "\n",
    "    synergy_vals = np.array([synergy_matrix[i, j] for (i, j) in pairs])\n",
    "\n",
    "    k = min(top_k, len(pairs))\n",
    "    top_synergy_idx = np.argsort(synergy_vals)[-k:]\n",
    "    top_synergy_pairs = set()\n",
    "    for idx in top_synergy_idx:\n",
    "        i, j = pairs[idx]\n",
    "        top_synergy_pairs.add((feature_indices[i], feature_indices[j]))\n",
    "\n",
    "    mi_sub = mi_values[feature_indices]\n",
    "    top_mi_feat = np.argsort(mi_sub)[-k:]\n",
    "    top_mi_pairs = set()\n",
    "    for i, j in combinations(sorted(top_mi_feat), 2):\n",
    "        top_mi_pairs.add((feature_indices[i], feature_indices[j]))\n",
    "\n",
    "    intersection = top_synergy_pairs & top_mi_pairs\n",
    "    union = top_synergy_pairs | top_mi_pairs\n",
    "    jaccard = len(intersection) / max(len(union), 1)\n",
    "\n",
    "    max_synergy_per_feat = np.max(synergy_matrix, axis=1)\n",
    "    mi_sub_arr = mi_values[feature_indices]\n",
    "    rho, pval = spearmanr(max_synergy_per_feat, mi_sub_arr) if len(mi_sub_arr) > 2 else (0.0, 1.0)\n",
    "\n",
    "    top_synergy_named = []\n",
    "    for idx in reversed(np.argsort(synergy_vals)[-min(5, len(synergy_vals)):]):\n",
    "        i, j = pairs[idx]\n",
    "        gi, gj = feature_indices[i], feature_indices[j]\n",
    "        top_synergy_named.append({\n",
    "            \"feature_i\": feature_names[gi], \"feature_j\": feature_names[gj],\n",
    "            \"synergy\": round(float(synergy_vals[idx]), 6),\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"jaccard_overlap\": round(jaccard, 4),\n",
    "        \"spearman_rho\": round(float(rho), 4) if not np.isnan(rho) else 0.0,\n",
    "        \"spearman_pval\": round(float(pval), 6) if not np.isnan(pval) else 1.0,\n",
    "        \"top_synergy_pairs\": top_synergy_named,\n",
    "        \"n_overlap\": len(intersection),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dvdoyce793q",
   "metadata": {},
   "source": [
    "## Stability Analysis & Synergy Graph\n",
    "\n",
    "Assess reproducibility by computing synergy on random subsamples, then build a synergy graph with edges between high-synergy pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u556poz2lcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stability_analysis(\n",
    "    X_disc: np.ndarray, y: np.ndarray, feature_names: list[str],\n",
    "    dataset_name: str, feature_indices: list[int],\n",
    "    n_subsamples: int = STABILITY_SUBSAMPLES,\n",
    "    subsample_frac: float = STABILITY_FRACTION,\n",
    ") -> dict:\n",
    "    \"\"\"Compute synergy matrices on random subsamples and assess stability.\"\"\"\n",
    "    n_feat = len(feature_indices)\n",
    "    n_pairs = n_feat * (n_feat - 1) // 2\n",
    "    n_samples = X_disc.shape[0]\n",
    "    subsample_size = int(n_samples * subsample_frac)\n",
    "\n",
    "    rng = np.random.RandomState(42)\n",
    "    upper_triangles = []\n",
    "\n",
    "    for s in range(n_subsamples):\n",
    "        indices = rng.choice(n_samples, size=subsample_size, replace=False)\n",
    "        X_sub, y_sub = X_disc[indices], y[indices]\n",
    "\n",
    "        syn_mat = np.zeros((n_feat, n_feat))\n",
    "        pairs = list(combinations(range(n_feat), 2))\n",
    "        for (li, lj) in pairs:\n",
    "            gi, gj = feature_indices[li], feature_indices[lj]\n",
    "            try:\n",
    "                d = build_trivariate_dist(X_sub[:, gi], X_sub[:, gj], y_sub)\n",
    "                result = PID_MMI(d)\n",
    "                syn = float(result.get_pi(((0, 1),)))\n",
    "                syn_mat[li, lj] = syn\n",
    "                syn_mat[lj, li] = syn\n",
    "            except Exception:\n",
    "                pass\n",
    "        upper_triangles.append([syn_mat[li, lj] for (li, lj) in pairs])\n",
    "\n",
    "    correlations = []\n",
    "    for i in range(n_subsamples):\n",
    "        for j in range(i + 1, n_subsamples):\n",
    "            if len(upper_triangles[i]) > 2:\n",
    "                rho, _ = spearmanr(upper_triangles[i], upper_triangles[j])\n",
    "                if not np.isnan(rho):\n",
    "                    correlations.append(rho)\n",
    "\n",
    "    mean_corr = float(np.mean(correlations)) if correlations else 0.0\n",
    "    std_corr = float(np.std(correlations)) if correlations else 0.0\n",
    "    print(f\"  Stability {dataset_name}: mean_rho={mean_corr:.4f} +/- {std_corr:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"n_subsamples\": n_subsamples, \"subsample_fraction\": subsample_frac,\n",
    "        \"mean_spearman\": round(mean_corr, 4), \"std_spearman\": round(std_corr, 4),\n",
    "        \"all_correlations\": [round(c, 4) for c in correlations],\n",
    "    }\n",
    "\n",
    "\n",
    "def build_synergy_graph(\n",
    "    synergy_matrix: np.ndarray, feature_names: list[str],\n",
    "    feature_indices: list[int], threshold_percentile: int = SYNERGY_THRESHOLD_PCTL,\n",
    ") -> dict:\n",
    "    \"\"\"Construct synergy graph from thresholded synergy matrix.\"\"\"\n",
    "    n_feat = len(feature_indices)\n",
    "    pairs = list(combinations(range(n_feat), 2))\n",
    "    syn_vals = [synergy_matrix[i, j] for (i, j) in pairs]\n",
    "\n",
    "    if not syn_vals or max(syn_vals) == 0:\n",
    "        return {\"threshold\": 0.0, \"n_edges\": 0, \"n_nodes\": n_feat,\n",
    "                \"n_components\": n_feat, \"largest_clique_size\": 0, \"top_5_edges\": []}\n",
    "\n",
    "    threshold = float(np.percentile(syn_vals, threshold_percentile))\n",
    "\n",
    "    G = nx.Graph()\n",
    "    for idx in range(n_feat):\n",
    "        G.add_node(feature_names[feature_indices[idx]])\n",
    "    for (li, lj) in pairs:\n",
    "        if synergy_matrix[li, lj] >= threshold:\n",
    "            gi, gj = feature_indices[li], feature_indices[lj]\n",
    "            G.add_edge(feature_names[gi], feature_names[gj],\n",
    "                       weight=synergy_matrix[li, lj])\n",
    "\n",
    "    n_components = nx.number_connected_components(G)\n",
    "    try:\n",
    "        cliques = list(nx.find_cliques(G))\n",
    "        largest_clique_size = max(len(c) for c in cliques) if cliques else 0\n",
    "    except Exception:\n",
    "        largest_clique_size = 0\n",
    "\n",
    "    edges_sorted = sorted(G.edges(data=True), key=lambda e: e[2].get(\"weight\", 0), reverse=True)\n",
    "    top_5 = [{\"feature_i\": e[0], \"feature_j\": e[1], \"synergy\": round(e[2][\"weight\"], 6)}\n",
    "             for e in edges_sorted[:5]]\n",
    "\n",
    "    return {\"threshold\": round(threshold, 6), \"n_edges\": G.number_of_edges(),\n",
    "            \"n_nodes\": G.number_of_nodes(), \"n_components\": n_components,\n",
    "            \"largest_clique_size\": largest_clique_size, \"top_5_edges\": top_5}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90q48n90big",
   "metadata": {},
   "source": [
    "## Run Full Pipeline\n",
    "\n",
    "Process each dataset: discretize → synergy matrix → MI comparison → stability → synergy graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781vjswbjd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_global_start = time.time()\n",
    "all_results = []\n",
    "\n",
    "for ds_name, ds_info in datasets.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing: {ds_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    X, y = ds_info[\"X\"], ds_info[\"y\"]\n",
    "    feature_names = ds_info[\"feature_names\"]\n",
    "    n_features = ds_info[\"n_features\"]\n",
    "\n",
    "    # Subsample features for large datasets\n",
    "    feature_indices = list(range(n_features))\n",
    "    if n_features > MAX_FEATURES_LARGE:\n",
    "        mi_all = mutual_info_classif(\n",
    "            discretize(X, n_bins=N_BINS), y, discrete_features=True, random_state=42\n",
    "        )\n",
    "        top_feat_idx = np.argsort(mi_all)[-MAX_FEATURES_LARGE:]\n",
    "        feature_indices = sorted(top_feat_idx.tolist())\n",
    "\n",
    "    # Discretize\n",
    "    X_disc = discretize(X, n_bins=N_BINS)\n",
    "\n",
    "    # Synergy matrix\n",
    "    synergy_result = compute_synergy_matrix(\n",
    "        X_disc=X_disc, y=y, feature_names=feature_names,\n",
    "        dataset_name=ds_name, feature_indices=feature_indices,\n",
    "    )\n",
    "\n",
    "    # MI comparison\n",
    "    mi_values = mutual_info_classif(X_disc, y, discrete_features=True, random_state=42)\n",
    "    mi_comparison = compare_synergy_vs_mi(\n",
    "        synergy_matrix=np.array(synergy_result[\"synergy_matrix\"]),\n",
    "        mi_values=mi_values, feature_names=feature_names,\n",
    "        feature_indices=feature_indices,\n",
    "    )\n",
    "\n",
    "    # Synergy graph\n",
    "    graph_result = build_synergy_graph(\n",
    "        synergy_matrix=np.array(synergy_result[\"synergy_matrix\"]),\n",
    "        feature_names=feature_names, feature_indices=feature_indices,\n",
    "    )\n",
    "\n",
    "    # Stability analysis\n",
    "    stability_result = stability_analysis(\n",
    "        X_disc=X_disc, y=y, feature_names=feature_names,\n",
    "        dataset_name=ds_name, feature_indices=feature_indices,\n",
    "    )\n",
    "\n",
    "    # MI values for features used\n",
    "    mi_dict = {feature_names[idx]: round(float(mi_values[idx]), 6)\n",
    "               for idx in feature_indices}\n",
    "\n",
    "    all_results.append({\n",
    "        \"dataset\": ds_name,\n",
    "        \"n_samples\": ds_info[\"n_samples\"], \"n_features\": n_features,\n",
    "        \"n_features_used\": len(feature_indices), \"n_classes\": ds_info[\"n_classes\"],\n",
    "        \"synergy\": synergy_result, \"mi_values\": mi_dict,\n",
    "        \"mi_comparison\": mi_comparison, \"synergy_graph\": graph_result,\n",
    "        \"stability\": stability_result,\n",
    "    })\n",
    "\n",
    "total_time = time.time() - t_global_start\n",
    "print(f\"\\nDone in {total_time:.1f}s — {len(all_results)} dataset(s) processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2i7asjn8um",
   "metadata": {},
   "source": [
    "## Results Visualization\n",
    "\n",
    "Display key results: PID decomposition heatmap, synergy vs Co-Information scatter, and summary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oi7xj11jicq",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in all_results:\n",
    "    ds_name = r[\"dataset\"]\n",
    "    syn_mat = np.array(r[\"synergy\"][\"synergy_matrix\"])\n",
    "    coi_mat = np.array(r[\"synergy\"][\"coi_matrix\"])\n",
    "    feat_indices = r[\"synergy\"][\"feature_indices_used\"]\n",
    "    feat_names = [r[\"mi_values\"] and list(r[\"mi_values\"].keys())]\n",
    "    feat_labels = list(r[\"mi_values\"].keys())\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    fig.suptitle(f\"PID Synergy Analysis: {ds_name}\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "    # 1. Synergy heatmap\n",
    "    ax = axes[0]\n",
    "    im = ax.imshow(syn_mat, cmap=\"YlOrRd\", aspect=\"auto\")\n",
    "    ax.set_xticks(range(len(feat_labels)))\n",
    "    ax.set_xticklabels(feat_labels, rotation=45, ha=\"right\", fontsize=8)\n",
    "    ax.set_yticks(range(len(feat_labels)))\n",
    "    ax.set_yticklabels(feat_labels, fontsize=8)\n",
    "    ax.set_title(\"Synergy Matrix\")\n",
    "    fig.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "    # 2. Synergy vs Co-Information scatter\n",
    "    ax = axes[1]\n",
    "    pairs = list(combinations(range(syn_mat.shape[0]), 2))\n",
    "    syn_vals = [syn_mat[i, j] for i, j in pairs]\n",
    "    coi_vals = [coi_mat[i, j] for i, j in pairs]\n",
    "    ax.scatter(coi_vals, syn_vals, alpha=0.7, edgecolors=\"k\", linewidth=0.5)\n",
    "    ax.set_xlabel(\"Co-Information (\\u2212CoI \\u2192 synergy)\")\n",
    "    ax.set_ylabel(\"PID Synergy\")\n",
    "    ax.set_title(\"Synergy vs Co-Information\")\n",
    "    ax.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "    ax.axvline(x=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    # 3. PID decomposition bar chart\n",
    "    ax = axes[2]\n",
    "    pid_details = r[\"synergy\"][\"pid_details\"]\n",
    "    pair_labels = list(pid_details.keys())[:6]  # Top 6 pairs\n",
    "    synergies = [pid_details[k][\"synergy\"] for k in pair_labels]\n",
    "    redundancies = [pid_details[k][\"redundancy\"] for k in pair_labels]\n",
    "    unique_0s = [pid_details[k][\"unique_0\"] for k in pair_labels]\n",
    "    unique_1s = [pid_details[k][\"unique_1\"] for k in pair_labels]\n",
    "    x = np.arange(len(pair_labels))\n",
    "    width = 0.2\n",
    "    ax.bar(x - 1.5*width, synergies, width, label=\"Synergy\", color=\"#e74c3c\")\n",
    "    ax.bar(x - 0.5*width, redundancies, width, label=\"Redundancy\", color=\"#3498db\")\n",
    "    ax.bar(x + 0.5*width, unique_0s, width, label=\"Unique\\u2080\", color=\"#2ecc71\")\n",
    "    ax.bar(x + 1.5*width, unique_1s, width, label=\"Unique\\u2081\", color=\"#f39c12\")\n",
    "    short_labels = [lbl.replace(\"__x__\", \"\\n\\u00d7\\n\") for lbl in pair_labels]\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(short_labels, fontsize=6, rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(\"Information (bits)\")\n",
    "    ax.set_title(\"PID Decomposition per Pair\")\n",
    "    ax.legend(fontsize=7, loc=\"upper right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Summary table\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Summary: {ds_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  PID method:           {r['synergy']['pid_method']}\")\n",
    "    print(f\"  Pairs computed:       {r['synergy']['completed_pairs']}/{r['synergy']['n_pairs']}\")\n",
    "    print(f\"  Errors:               {r['synergy']['errors']}\")\n",
    "    print(f\"  Computation time:     {r['synergy']['total_time_s']:.2f}s\")\n",
    "    print(f\"  Jaccard overlap:      {r['mi_comparison']['jaccard_overlap']:.4f}\")\n",
    "    print(f\"  Spearman rho:         {r['mi_comparison']['spearman_rho']:.4f}\")\n",
    "    print(f\"  Graph edges:          {r['synergy_graph']['n_edges']}\")\n",
    "    print(f\"  Graph components:     {r['synergy_graph']['n_components']}\")\n",
    "    print(f\"  Largest clique:       {r['synergy_graph']['largest_clique_size']}\")\n",
    "    if r[\"stability\"]:\n",
    "        print(f\"  Stability rho:        {r['stability']['mean_spearman']:.4f} \\u00b1 {r['stability']['std_spearman']:.4f}\")\n",
    "    print(f\"\\n  Top synergy pairs:\")\n",
    "    for p in r[\"mi_comparison\"][\"top_synergy_pairs\"][:5]:\n",
    "        print(f\"    {p['feature_i']:25s} \\u00d7 {p['feature_j']:25s} \\u2192 {p['synergy']:.6f}\")\n",
    "\n",
    "    # XOR validation summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"XOR Validation Results\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for k, v in xor_results.items():\n",
    "        print(f\"  {k:30s}: {v}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}